{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uproot4\n",
    "from deserialization_hacks import tree_arrays\n",
    "import awkward1 as ak\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read branches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_branch(branch):\n",
    "        k = branch.name\n",
    "\n",
    "        if not \"Aux\" in k:\n",
    "            return False\n",
    "\n",
    "        # the following don't contain data (in split files)\n",
    "        if k.endswith(\".\"):\n",
    "            return\n",
    "        if \"SG::\" in k:\n",
    "            return\n",
    "        if k.endswith(\"Base\"):\n",
    "            return\n",
    "\n",
    "        # are often empty\n",
    "        # see https://github.com/scikit-hep/uproot4/issues/126\n",
    "        if \"DescrTags\" in k:\n",
    "            return\n",
    "    \n",
    "        interpretation = str(branch.interpretation)\n",
    "\n",
    "        # skip triple-jagged vectors and sets\n",
    "        if (interpretation.count(\"AsVector\") > 1) and (\"AsString\" in interpretation):\n",
    "            return False\n",
    "        if interpretation.count(\"AsVector\") > 2:\n",
    "            return False\n",
    "        if \"AsSet\" in interpretation:\n",
    "            # what are these anyways?\n",
    "            return False\n",
    "        \n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.53 s, sys: 1.23 s, total: 10.8 s\n",
      "Wall time: 8.13 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# file from user.nihartma.physlite_test_ttbar_split99.001_EXT0\n",
    "f = uproot4.open(\"user.nihartma.22884623.EXT0._000001.DAOD_PHYSLITE.test.pool.root\")\n",
    "tree = f[\"CollectionTree\"]\n",
    "array_dict = tree_arrays(tree, filter_branch=filter_branch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1117"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(array_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What did we miss (assuming fully split Aux branches)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'EventInfoAux.detDescrTags.first',\n",
       " 'EventInfoAux.detDescrTags.second',\n",
       " 'EventInfoAuxDyn.streamTagDets',\n",
       " 'EventInfoAuxDyn.streamTagRobs',\n",
       " 'METAssoc_AnalysisMETAux.overlapIndices',\n",
       " 'METAssoc_AnalysisMETAux.overlapTypes'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_aux = [\n",
    "    k.split(\"/\")[-1] for k in tree.keys(\"/(.*Aux\\..+|.*AuxDyn\\..+)/i\")\n",
    "    if not \"xAOD::\" in k\n",
    "    and len(tree[k].branches) == 0\n",
    "]\n",
    "set(all_aux).difference(array_dict.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write to parquet\n",
    "We can make a flat dictionary of branches (but the branches may be jagged arrays). That's something we can store in parquet already now. As of now parquet can't store jagged arrays of structs, so those need to be exploded as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_exploded = {}\n",
    "for key, array in array_dict.items():\n",
    "    keys = ak.keys(array)\n",
    "    if len(keys) == 0:\n",
    "        d_exploded[key] = array\n",
    "    for subkey in keys:\n",
    "        d_exploded[f\"{key}.{subkey}\"] = array[subkey]\n",
    "\n",
    "Events_flat = ak.zip(d_exploded, depth_limit=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ak.to_parquet(Events_flat, \"physlite.parquet\", explode_records=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this gives us already in the default settings a fast readable format with about the same size that these branches originally took on disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120.44293880462646"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.stat(\"physlite.parquet\").st_size / (1024 ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "117.07681941986084"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([tree[k].compressed_bytes for k in array_dict]) / (1024 ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.09 s, sys: 438 ms, total: 2.53 s\n",
      "Wall time: 521 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Array [...] type='10000 * {\"EventInfoAux.runNumber\": ?int64, \"EventInfoAux.even...'>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "ak.from_parquet(\"physlite.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build an nicer structure\n",
    "\n",
    "We can also structure this much nicer and remove duplicated indices (e.g. all electron properties share the same offsets) - the naming conventions help us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regroup(array_dict):\n",
    "    regrouped = {}\n",
    "    for k_top in set(k.split(\".\")[0] for k in array_dict):\n",
    "        # zip will put together jagged arrays with common offsets\n",
    "        def ak_zip(depth_limit=2):\n",
    "            return ak.zip(\n",
    "                {k.replace(k_top, \"\")[1:] : array_dict[k] for k in array_dict if k_top in k},\n",
    "                depth_limit=depth_limit\n",
    "            )\n",
    "        # for some containers this will work 2 levels, for some only up to 1\n",
    "        try:\n",
    "            v = ak_zip(depth_limit=2)\n",
    "        except ValueError:\n",
    "            v = ak_zip(depth_limit=1)\n",
    "        regrouped[k_top.replace(\"AuxDyn\", \"\").replace(\"Aux\", \"\")] = v\n",
    "    # lets restructure such that we get TrigMatchedObjets.<trigger-name>\n",
    "    # instead of AnalysisHLT_<trigger_name>.TrigMatchedObjects\n",
    "    trig_matched_objects = ak.zip(\n",
    "        {\n",
    "            k.replace(\"AnalysisHLT_\", \"\") : regrouped[k].TrigMatchedObjects\n",
    "            for k in regrouped if \"AnalysisHLT\" in k\n",
    "        },\n",
    "        depth_limit=1\n",
    "    )\n",
    "    for k in list(regrouped.keys()):\n",
    "        if \"AnalysisHLT\" in k:\n",
    "            regrouped.pop(k)\n",
    "    regrouped[\"TrigMatchedObjects\"] = trig_matched_objects\n",
    "    return ak.zip(regrouped, depth_limit=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "Events = regroup(array_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Array [[], [3.37e+03], ... [6.27e+03]] type='10000 * var * float32'>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Events.AnalysisElectrons.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Array [[], [-0.861], ... [-1.21], [-0.238]] type='10000 * var * float32'>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Events.AnalysisElectrons.eta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The total in-memory size got a bit smaller due to the removed duplicated indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "360.5864849090576"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Events.nbytes / (1024 ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "401.86948585510254"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Events_flat.nbytes / (1024 ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Store in HDF5\n",
    "\n",
    "One can store this in basically any data format using `ak.to_arrayset` which will separate the underlying 1-d arrays (content and indices for jagged arrays) and a json spec for the structure. See https://awkward-array.org/how-to-convert-arrayset.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "form, container, num_partitions = ak.to_arrayset(Events)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HDF5 is rather well suited for this since we can put the json form directly into the metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(\"physlite.h5\", \"w\") as file:\n",
    "    group = file.create_group(\"awkward\")\n",
    "    for k in container:\n",
    "        v = container[k]\n",
    "        group.create_dataset(k, shape=v.shape, dtype=v.dtype, data=v, compression=\"gzip\")\n",
    "    group.attrs[\"form\"] = form.tojson()\n",
    "    group.attrs[\"length\"] = len(Events)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file is quite a bit smaller. That makes sense since the compression can't compress away duplicated indices of different split branches (which we had before in the ROOT or parquet file)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "89.55333232879639"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.stat(\"physlite.h5\").st_size / (1024 ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading is also quite fast, although not as fast as with the parquet file (can be improved by using a faster compression, e.g. \"lzf\" at the cost of larger file size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.66 s, sys: 116 ms, total: 1.77 s\n",
      "Wall time: 1.78 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with h5py.File(\"physlite.h5\", \"r\") as file:\n",
    "    group = file[\"awkward\"]\n",
    "    reconstituted = ak.from_arrayset(\n",
    "        ak.forms.Form.fromjson(group.attrs[\"form\"]),\n",
    "        {k: np.asarray(v) for k, v in group.items()},\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading only requested columns also works e.g. via \"LazyArrays\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LazyGet:\n",
    "    def __init__(self, group):\n",
    "        self.group = group\n",
    "    \n",
    "    def __getitem__(self, key):\n",
    "        print(f\"Reading array {key}\")\n",
    "        return np.asarray(self.group[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = h5py.File(\"physlite.h5\", \"r\")\n",
    "group = file[\"awkward\"]\n",
    "\n",
    "lazy = ak.from_arrayset(\n",
    "    ak.forms.Form.fromjson(group.attrs[\"form\"]),\n",
    "    LazyGet(group),\n",
    "    lazy=True,\n",
    "    lazy_lengths=group.attrs[\"length\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 µs, sys: 0 ns, total: 4 µs\n",
      "Wall time: 7.39 µs\n",
      "Reading array node958-offsets\n",
      "Reading array node964\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Array [[], [3.37e+03], ... [6.27e+03]] type='10000 * var * float32'>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time\n",
    "lazy.AnalysisElectrons.pt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Behaviour and dynamic quantities\n",
    "Working in a bit more object-oriented way can be done with \"behaviours\" (see https://awkward-array.readthedocs.io/en/latest/ak.behavior.html).\n",
    "\n",
    "For example, coffea has LorentzVectors for awkward array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import coffea\n",
    "from coffea.nanoevents.methods import vector\n",
    "ak.behavior.update(vector.behavior)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The coffea `PtEtaPhiELorentzVector` calls the mass `mass`, but we call it `m`, so let's override that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ak.mixin_class(ak.behavior)\n",
    "class xAODParticle(vector.PtEtaPhiELorentzVector):\n",
    "    @property\n",
    "    def mass(self):\n",
    "        return self.m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, if name our Particles \"xAODParticle\" we can do all the LorentzVector stuff with them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "for collection in [\"Electrons\", \"Jets\", \"Photons\", \"Muons\"]:\n",
    "    Events[f\"Analysis{collection}\"].layout.content.setparameter(\"__record__\", \"xAODParticle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xAODParticleArray [[], ... firstEgMotherPdgId: -11}]] type='10000 * var * xAODP...'>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Events.AnalysisElectrons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Array [[], [0.104], ... [0.0422], [0.943]] type='10000 * var * ?float32'>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Events.AnalysisElectrons.delta_r(Events.AnalysisElectrons.nearest(Events.AnalysisJets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or something for track Particles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ak.mixin_class(ak.behavior)\n",
    "class xAODTrackParticle(vector.LorentzVector):\n",
    "    \"see https://gitlab.cern.ch/atlas/athena/-/blob/21.2/Event/xAOD/xAODTracking/Root/TrackParticle_v1.cxx#L82\"\n",
    "    @property\n",
    "    def theta(self):\n",
    "        return self[\"theta\"]\n",
    "    \n",
    "    @property\n",
    "    def phi(self):\n",
    "        return self[\"phi\"]\n",
    "\n",
    "    @property\n",
    "    def p(self):\n",
    "        return 1. / np.abs(self.qOverP)\n",
    "    \n",
    "    @property\n",
    "    def x(self):\n",
    "        return self.p * np.sin(self.theta) * np.cos(self.phi)\n",
    "    \n",
    "    @property\n",
    "    def y(self):\n",
    "        return self.p * np.sin(self.theta) * np.sin(self.phi)\n",
    "\n",
    "    @property\n",
    "    def z(self):\n",
    "        return self.p * np.cos(self.theta)\n",
    "    \n",
    "    @property\n",
    "    def t(self):\n",
    "        return np.sqrt(139.570 ** 2 + sef.x ** 2 + self.y ** 2 + self.z ** 2)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in ak.keys(Events):\n",
    "    if not \"TrackParticles\" in k:\n",
    "        continue\n",
    "    Events[k].layout.content.setparameter(\"__record__\", \"xAODTrackParticle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xAODTrackParticleArray [[{phi: 2.36, ... ] type='10000 * var * xAODTrackParticl...'>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Events.InDetTrackParticles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Array [[1.11e+05, 1.7e+04, ... 5.57e+03, 739]] type='10000 * var * float32'>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Events.InDetTrackParticles.pt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ElementLinks\n",
    "\n",
    "Non-cyclic references can be implemented by just adding new indices and reusing the same contents. E.g let's link Electrons to their track particles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def element_links(collection1, links, collection2):\n",
    "    # Note: For proper handling one should read the\n",
    "    # EventFormat.m_branchNames, EventFormat.m_branchHashes mapping to link to the correct collection\n",
    "    # (possibly use UnionArray)\n",
    "    # Also one could see if there is a better way to replicate the exact structure\n",
    "    # instead of hardcoding\n",
    "    return ak.Array(\n",
    "        ak.layout.ListOffsetArray64(\n",
    "            collection1.layout.offsets,\n",
    "            ak.layout.ListArray64(\n",
    "                links.layout.content.starts,\n",
    "                links.layout.content.stops,\n",
    "                ak.layout.IndexedArray64(\n",
    "                    ak.layout.Index64(\n",
    "                        ak.flatten(\n",
    "                            links.m_persIndex\n",
    "                            + ak.Array(np.array(collection2.layout.offsets[:-1])),\n",
    "                            axis=None\n",
    "                        )\n",
    "                    ),\n",
    "                    collection2.layout.content\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "Events[\"AnalysisElectrons\", \"trackParticles\"] = element_links(\n",
    "    Events.AnalysisElectrons,\n",
    "    Events.AnalysisElectrons.trackParticleLinks,\n",
    "    Events.GSFTrackParticles\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xAODTrackParticleArray [[], ... chiSquared: 68}]]] type='10000 * var * var * xA...'>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Events.AnalysisElectrons.trackParticles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Array [[], [4.13e+03], ... [5.68e+03]] type='10000 * var * float32'>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first track particle pt for each electron\n",
    "Events.AnalysisElectrons.trackParticles[:,:,0].pt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

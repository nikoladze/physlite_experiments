{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uproot\n",
    "from physlite_experiments import tree_arrays\n",
    "import awkward as ak\n",
    "import numpy as np\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read branches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_branch(branch):\n",
    "        k = branch.name\n",
    "\n",
    "        if not \"Aux\" in k:\n",
    "            return False\n",
    "\n",
    "        # the following don't contain data (in split files)\n",
    "        if k.endswith(\".\"):\n",
    "            return False\n",
    "        if \"SG::\" in k:\n",
    "            return False\n",
    "        if k.endswith(\"Base\"):\n",
    "            return False\n",
    "\n",
    "        # are often empty\n",
    "        # see https://github.com/scikit-hep/uproot4/issues/126\n",
    "        # -> now fixed, but my custom deserialization does not work yet with them\n",
    "        if \"DescrTags\" in k:\n",
    "            return False\n",
    "        \n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.78 s, sys: 955 ms, total: 7.73 s\n",
      "Wall time: 6.75 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# file from user.nihartma.physlite_test_ttbar_split99.001_EXT0\n",
    "f = uproot.open(\"user.nihartma.22884623.EXT0._000001.DAOD_PHYSLITE.test.pool.root\")\n",
    "tree = f[\"CollectionTree\"]\n",
    "array_dict = tree_arrays(tree, filter_branch=filter_branch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1121"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(array_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What did we miss (assuming fully split Aux branches)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'EventInfoAux.detDescrTags.first', 'EventInfoAux.detDescrTags.second'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_aux = [\n",
    "    k.split(\"/\")[-1] for k in tree.keys(\"/(.*Aux\\..+|.*AuxDyn\\..+)/i\")\n",
    "    if not \"xAOD::\" in k\n",
    "    and len(tree[k].branches) == 0\n",
    "]\n",
    "set(all_aux).difference(array_dict.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build an nicer structure\n",
    "\n",
    "We can also structure this much nicer and remove duplicated indices (e.g. all electron properties share the same offsets) - the naming conventions help us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regroup(array_dict):\n",
    "    regrouped = {}\n",
    "    for k_top in set(k.split(\".\")[0] for k in array_dict):\n",
    "        if k_top == \"EventInfoAux\":\n",
    "            # skip that for now - let's use EventInfoAuxDyn\n",
    "            continue\n",
    "        if k_top == \"EventInfoAuxDyn\":\n",
    "            k_top = \"EventInfoAux\"\n",
    "        # zip will put together jagged arrays with common offsets\n",
    "        def ak_zip(depth_limit=2):\n",
    "            return ak.zip(\n",
    "                {k.replace(k_top, \"\")[1:] : array_dict[k] for k in array_dict if k_top in k},\n",
    "                depth_limit=depth_limit\n",
    "            )\n",
    "        # for some containers this will work 2 levels, for some only up to 1\n",
    "        try:\n",
    "            v = ak_zip(depth_limit=2)\n",
    "        except ValueError:\n",
    "            v = ak_zip(depth_limit=1)\n",
    "        regrouped[k_top.replace(\"AuxDyn\", \"\").replace(\"Aux\", \"\")] = v\n",
    "    # lets restructure such that we get TrigMatchedObjets.<trigger-name>\n",
    "    # instead of AnalysisHLT_<trigger_name>.TrigMatchedObjects\n",
    "    trig_matched_objects = ak.zip(\n",
    "        {\n",
    "            k.replace(\"AnalysisHLT_\", \"\") : regrouped[k].TrigMatchedObjects\n",
    "            for k in regrouped if \"AnalysisHLT\" in k\n",
    "        },\n",
    "        depth_limit=1\n",
    "    )\n",
    "    for k in list(regrouped.keys()):\n",
    "        if \"AnalysisHLT\" in k:\n",
    "            regrouped.pop(k)\n",
    "    regrouped[\"TrigMatchedObjects\"] = trig_matched_objects\n",
    "    return ak.zip(regrouped, depth_limit=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Events = regroup(array_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Array [[], [3.37e+03], ... [6.27e+03]] type='10000 * var * float32'>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Events.AnalysisElectrons.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Array [[], [-0.861], ... [-1.21], [-0.238]] type='10000 * var * float32'>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Events.AnalysisElectrons.eta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The total in-memory size got a bit smaller due to the removed duplicated indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "322.89591217041016"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Events.nbytes / (1024 ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "404.93270111083984"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ak.Array(array_dict).nbytes / (1024 ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write to parquet\n",
    "Since `pyarrow>=2` we can write such a structure directly to parquet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ak.to_parquet(Events, \"physlite.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this gives us already in the default settings a fast readable format with about the same size that these branches originally took on disk. The reason why it is larger (or at least not smaller, despite deduplicated offsets) could be that the default compression in parquet is a bit less disk size efficient and more optimized for fast reading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "112.27896118164062"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.stat(\"physlite.parquet\").st_size / (1024 ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "117.29965114593506"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([tree[k].compressed_bytes for k in array_dict]) / (1024 ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "597 ms ± 12.7 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "ak.from_parquet(\"physlite.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also be read this as a lazy array, such that branches are only read on demand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 62.9 ms, sys: 12.4 ms, total: 75.3 ms\n",
      "Wall time: 65.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "lazy_parquet = ak.from_parquet(\"physlite.parquet\", lazy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Array [[1.07e+05], [], ... [], [1.26e+05]] type='10000 * var * float32'>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lazy_parquet.AnalysisMuons.pt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `awkward` tag [1.1.0rc1](https://github.com/scikit-hep/awkward-1.0/releases/tag/1.1.0rc1) with `pyarrow>=3` this actually only caches the Muon pt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'ak.from_parquet:0:off:AnalysisMuons.list.item.pt[0]': <ListOffsetArray64>\n",
       "     <offsets><Index64 i=\"[0 1 1 1 1 ... 5302 5302 5302 5302 5303]\" offset=\"0\" length=\"10001\" at=\"0x7f3a2b354540\"/></offsets>\n",
       "     <content><RecordArray>\n",
       "         <field index=\"0\" key=\"pt\">\n",
       "             <NumpyArray format=\"f\" shape=\"5303\" data=\"106870 25507.7 47097 74740.1 9129.62 ... 119823 60685.7 64930.9 37034.6 125925\" at=\"0x7f3a2b34d800\"/>\n",
       "         </field>\n",
       "     </RecordArray></content>\n",
       " </ListOffsetArray64>, 'ak.from_parquet:0:col:AnalysisMuons.list.item.pt[0]': <NumpyArray format=\"f\" shape=\"5303\" data=\"106870 25507.7 47097 74740.1 9129.62 ... 119823 60685.7 64930.9 37034.6 125925\" at=\"0x7f3a23c7eec0\"/>},)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lazy_parquet._caches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write to HDF5 or other formats\n",
    "\n",
    "One can store awkward arrays in basically any data format using `ak.to_buffers` which will separate the underlying 1-d arrays (content and indices for jagged arrays) and a json spec for the structure. See https://awkward-array.org/how-to-convert-buffers.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "form, length, container = ak.to_buffers(Events)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HDF5 is rather well suited for this since we can put the json form directly into the metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(\"physlite.h5\", \"w\") as file:\n",
    "    group = file.create_group(\"awkward\")\n",
    "    for k in container:\n",
    "        v = container[k]\n",
    "        group.create_dataset(k, shape=v.shape, dtype=v.dtype, data=v, compression=\"lzf\")\n",
    "    group.attrs[\"form\"] = form.tojson()\n",
    "    group.attrs[\"length\"] = length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file is quite a bit smaller. That makes sense since the compression can't compress away duplicated indices of different split branches (which we had before in the ROOT file)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "105.9586591720581"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.stat(\"physlite.h5\").st_size / (1024 ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading is also quite fast, although not as fast as with the parquet file (can be improved by using a faster compression, e.g. \"lzf\" at the cost of larger file size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 885 ms, sys: 86.4 ms, total: 971 ms\n",
      "Wall time: 971 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with h5py.File(\"physlite.h5\", \"r\") as file:\n",
    "    group = file[\"awkward\"]\n",
    "    reconstituted = ak.from_buffers(\n",
    "        ak.forms.Form.fromjson(group.attrs[\"form\"]),\n",
    "        group.attrs[\"length\"],\n",
    "        {k: np.asarray(v) for k, v in group.items()},\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading only requested columns also works e.g. via \"LazyArrays\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LazyGet:\n",
    "    def __init__(self, group):\n",
    "        self.group = group\n",
    "    \n",
    "    def __getitem__(self, key):\n",
    "        print(f\"Reading array {key}\")\n",
    "        return np.asarray(self.group[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = h5py.File(\"physlite.h5\", \"r\")\n",
    "group = file[\"awkward\"]\n",
    "\n",
    "lazy = ak.from_buffers(\n",
    "    ak.forms.Form.fromjson(group.attrs[\"form\"]),\n",
    "    group.attrs[\"length\"],\n",
    "    LazyGet(group),\n",
    "    lazy=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading array part0-node917-offsets\n",
      "Reading array part0-node923-data\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Array [[], [3.37e+03], ... [6.27e+03]] type='10000 * var * float32'>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lazy.AnalysisElectrons.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading array part0-node960-data\n",
      "Reading array part0-node939-data\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Array [[{DFCommonElectronsECIDS: 0, ... ] type='4103 * var * {\"DFCommonElectron...'>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lazy.AnalysisElectrons[ak.num(lazy.AnalysisElectrons.pt) > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One could also use very simple formats like `npz` (would need to store the metadata separately, e.g. in a json file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez_compressed(\"physlite.npz\", **container)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "77.33356857299805"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.stat(\"physlite.npz\").st_size / (1024 ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.53 s, sys: 70.1 ms, total: 1.6 s\n",
      "Wall time: 1.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with np.load(\"physlite.npz\") as npf:\n",
    "    reconstituted_np = ak.from_buffers(\n",
    "        form,\n",
    "        length,\n",
    "        {k: v for k, v in npf.items()},\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#file.close() # execute this to close the h5 file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Behaviour and dynamic quantities\n",
    "Working in a bit more object-oriented way can be done with \"behaviours\" (see https://awkward-array.readthedocs.io/en/latest/ak.behavior.html).\n",
    "\n",
    "For example, coffea has LorentzVectors for awkward array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from coffea.nanoevents.methods import vector\n",
    "ak.behavior.update(vector.behavior)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The coffea `PtEtaPhiELorentzVector` calls the mass `mass`, but we call it `m`, so let's override that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ak.mixin_class(ak.behavior)\n",
    "class xAODParticle(vector.PtEtaPhiELorentzVector):\n",
    "    @property\n",
    "    def mass(self):\n",
    "        return self.m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, if name our Particles \"xAODParticle\" we can do all the LorentzVector stuff with them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "for collection in [\"Electrons\", \"Jets\", \"Photons\", \"Muons\"]:\n",
    "    Events[f\"Analysis{collection}\"].layout.content.setparameter(\"__record__\", \"xAODParticle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xAODParticleArray [[], ... firstEgMotherPdgId: -11}]] type='10000 * var * xAODP...'>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Events.AnalysisElectrons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Array [[], [0.104], ... [0.0422], [0.943]] type='10000 * var * ?float32'>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Events.AnalysisElectrons.delta_r(Events.AnalysisElectrons.nearest(Events.AnalysisJets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or something for track Particles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ak.mixin_class(ak.behavior)\n",
    "class xAODTrackParticle(vector.LorentzVector):\n",
    "    \"see https://gitlab.cern.ch/atlas/athena/-/blob/21.2/Event/xAOD/xAODTracking/Root/TrackParticle_v1.cxx#L82\"\n",
    "    @property\n",
    "    def theta(self):\n",
    "        return self[\"theta\"]\n",
    "    \n",
    "    @property\n",
    "    def phi(self):\n",
    "        return self[\"phi\"]\n",
    "\n",
    "    @property\n",
    "    def p(self):\n",
    "        return 1. / np.abs(self.qOverP)\n",
    "    \n",
    "    @property\n",
    "    def x(self):\n",
    "        return self.p * np.sin(self.theta) * np.cos(self.phi)\n",
    "    \n",
    "    @property\n",
    "    def y(self):\n",
    "        return self.p * np.sin(self.theta) * np.sin(self.phi)\n",
    "\n",
    "    @property\n",
    "    def z(self):\n",
    "        return self.p * np.cos(self.theta)\n",
    "    \n",
    "    @property\n",
    "    def t(self):\n",
    "        return np.sqrt(139.570 ** 2 + sef.x ** 2 + self.y ** 2 + self.z ** 2)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in Events.fields:\n",
    "    if not \"TrackParticles\" in k:\n",
    "        continue\n",
    "    Events[k].layout.content.setparameter(\"__record__\", \"xAODTrackParticle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xAODTrackParticleArray [[{phi: 2.36, ... ] type='10000 * var * xAODTrackParticl...'>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Events.InDetTrackParticles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Array [[1.11e+05, 1.7e+04, ... 5.57e+03, 739]] type='10000 * var * float32'>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Events.InDetTrackParticles.pt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ElementLinks\n",
    "\n",
    "Non-cyclic references can be implemented by just adding new indices and reusing the same contents. E.g let's link Electrons to their track particles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def element_links(collection1, links, collection2):\n",
    "    # Note: For proper handling one should read the\n",
    "    # EventFormat.m_branchNames, EventFormat.m_branchHashes mapping to link to the correct collection\n",
    "    # (possibly use UnionArray)\n",
    "    # Also one could see if there is a better way to replicate the exact structure\n",
    "    # instead of hardcoding\n",
    "    return ak.Array(\n",
    "        ak.layout.ListOffsetArray64(\n",
    "            collection1.layout.offsets,\n",
    "            ak.layout.ListArray64(\n",
    "                links.layout.content.starts,\n",
    "                links.layout.content.stops,\n",
    "                ak.layout.IndexedArray64(\n",
    "                    ak.layout.Index64(\n",
    "                        ak.flatten(\n",
    "                            links.m_persIndex\n",
    "                            + ak.Array(np.array(collection2.layout.offsets[:-1])),\n",
    "                            axis=None\n",
    "                        )\n",
    "                    ),\n",
    "                    collection2.layout.content\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "Events[\"AnalysisElectrons\", \"trackParticles\"] = element_links(\n",
    "    Events.AnalysisElectrons,\n",
    "    Events.AnalysisElectrons.trackParticleLinks,\n",
    "    Events.GSFTrackParticles\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xAODTrackParticleArray [[], ... chiSquared: 68}]]] type='10000 * var * var * xA...'>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Events.AnalysisElectrons.trackParticles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Array [[], [4.13e+03], ... [5.68e+03]] type='10000 * var * float32'>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first track particle pt for each electron\n",
    "Events.AnalysisElectrons.trackParticles[:,:,0].pt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
